# -*- coding: utf-8 -*-
"""Linear Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/azfar154/34b6984af79c0898b37a119f8a2000aa/linear-regression.ipynb

**IMPORTANT READ ME!!!**
You must connect to a hosted gpu don't use your local environment. 
You can choose to run programs using GPU acceleration or with a TPU



Dark Mode is amazing so like on the top bar go to Tools then Preferences then change the mode to dark.

Import important Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib notebook
import numpy as np
import pandas as pd
import seaborn as sn
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

"""Make a Linear Regression Set"""

from sklearn.datasets import make_regression
X_R1,y_R1=make_regression(n_samples=100,n_features=1,n_informative=1,bias=150.0,noise=30,random_state=0)

"""Make a 75% test and 25% train ratio"""

X_train,x_test,y_train,y_test=train_test_split(X_R1,y_R1,random_state=0)

"""Linear Regression Least Squares"""

from sklearn.linear_model import LinearRegression
#Normal Linear Regression doesn't need an alpha paramter
linreg=LinearRegression().fit(X_train,y_train)
print("The linear coef/w hat is {}".format(linreg.coef_))
print("The intercept/bias term is {}".format(linreg.intercept_))
print("The score for the training data is {:.2f}".format(linreg.score(X_train,y_train)))
print("The score for the testing data is {:.2f}".format(linreg.score(x_test,y_test)))

"""Plotting this Linear Regression"""

plt.figure(figsize=(5,4))
plt.title("Linear Regression:Least Squares")
plt.scatter(X_R1,y_R1,marker='o',s=50,alpha=0.8)
plt.plot(X_R1,X_R1*linreg.coef_+linreg.intercept_,'r-')

"""Ridge Regression"""

from sklearn.linear_model import Ridge
## Ridge Regression uses the L2 penalty which features normalization this is essential when you have features that have a huge impact on the y hat which is the output.
#L2 penalty helps the user for overfitting. Overfitting only benefits the training data.
ridgeregress=Ridge(alpha=20).fit(X_train,y_train)
print('ridge regression linear model intercept: {}'
     .format(ridgeregress.intercept_))
print('ridge regression linear model coeff:\n{}'
     .format(ridgeregress.coef_))
print('R-squared score (training): {:.3f}'
     .format(ridgeregress.score(X_train, y_train)))
print('R-squared score (test): {:.3f}'
     .format(ridgeregress.score(x_test, y_test)))
print('Number of non-zero features: {}'.format(np.sum(ridgeregress.coef_ !=0)))

"""Finding the "best" alpha variable"""

for i in [1,10,20,50,100,200]:
  ridgeregress=Ridge(alpha=i).fit(X_train,y_train)
  print("When the alpha is",i)
  print("The score is {:.2f} for the training data".format(ridgeregress.score(X_train,y_train)))
  print("The score is {:.2f} for the training data".format(ridgeregress.score(x_test,y_test)))

"""Ridge Regression with feature normalization"""

#Scaler helps normalize the training and the test data we can use the MinMaxScaler)
scaler=MinMaxScaler()
x_train_scaled=scaler.fit_transform(X_train)
x_test_scaled=scaler.transform(x_test)
ridgeregressnorm=Ridge(alpha=20.0).fit(x_train_scaled,y_train)

print("Ridge regress noralization scores. Test:{:.2f} \n Training {:.2f}".format(ridgeregressnorm.score(x_train_scaled,y_train),ridgeregressnorm.score(x_test_scaled,y_test)))

"""Normalization will help your data.
LOOK AT THE DIFFERENCE BETWEEN THESE GRAPHS


https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02

If you want to find the amount of time to run a cell you can use the function %%time
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 6
# scaler=MinMaxScaler()
# x_train_scaled=scaler.fit_transform(X_train)
# x_test_scaled=scaler.transform(x_test)
# ridgeregressnorm=Ridge(alpha=20.0).fit(x_train_scaled,y_train)
# 
# print("Ridge regress noralization scores. Test:{:.2f} \n Training {:.2f}".format(ridgeregressnorm.score(x_train_scaled,y_train),ridgeregressnorm.score(x_test_scaled,y_test)))

"""Without an external gpu it is usually impossible to achieve such speeds.

Formula:

Input=(x0,x1...x n)

Function:
y hat= w hat 0 x 0 ... w hat n x n +b hat

y hat is the output

w hat is the model coefficent/slope

b hat is the y intercept

Least Squares
LLS=(y-(w i*x+b)
"""